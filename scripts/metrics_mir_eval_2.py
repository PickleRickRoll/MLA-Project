# -*- coding: utf-8 -*-
"""metrics mir_eval 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_b0ECSx_Loqml1lrTc-sitWGyuBXTx00
"""

!pip install --upgrade mir_eval

from typing import Any, Callable, Dict
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import ReLU
import tensorflow.keras.layers as nn
from tensorflow.keras.layers import concatenate
from utils import cqt , harmonic_stack , dsp ,vis_cqt
import numpy as np
import librosa
import mir_eval
import matplotlib.pyplot as plt

from modelv& import model_v1

def mir_eval (Yo, Yp, Yn):
    # Paramètres
    sr = 44100
    

    # Post-traitement pour obtenir les notes estimées
    onset_threshold = 0.5
    note_threshold = 0.5

    estimated_notes = []
    for f in range(Yo.shape[0]):
        onsets = np.where(Yo[f] > onset_threshold)[0]
        for onset in onsets:
            offset = onset + np.argmax(Yn[f, onset:] < note_threshold)
            if offset == onset:
                offset = len(Yn[f])
            # Get the frequency
            freq = librosa.fft_frequencies(sr=sr)[f]
            # Filter out frequencies that are too low
            if freq > 0:  # or some other minimum frequency threshold
                estimated_notes.append((librosa.frames_to_time(onset), 
                                        librosa.frames_to_time(offset), 
                                        freq))

    notes = estimated_notes

    # Évaluation avec mir_eval
    ref_intervals = np.array([[start, end] for start, end, _ in notes])

    ref_pitches = np.array([librosa.hz_to_midi(freq) for _, _, freq in notes])


    est_intervals = np.array([[start, end] for start, end, _ in estimated_notes])
    est_pitches = np.array([librosa.hz_to_midi(freq) for _, _, freq in estimated_notes])

    onset_tolerance = 0.5  # 50 ms
    pitch_tolerance = 0.25  # quart de ton
    offset_ratio = 0.2 # 20% de la durée de la note
    
    # F-measure
    precision, recall, f_measure, average_overlap_ratio = mir_eval.transcription.precision_recall_f1_overlap(
        ref_intervals, ref_pitches, est_intervals, est_pitches,
        onset_tolerance=onset_tolerance, pitch_tolerance=pitch_tolerance,
        offset_ratio=offset_ratio
    )
    
    # F-measure no offset
    precision_no, recall_no, f_measure_no,  average_overlap_ratio_no  = mir_eval.transcription.precision_recall_f1_overlap(
    ref_intervals, ref_pitches, est_intervals, est_pitches,
    onset_tolerance=onset_tolerance, pitch_tolerance=pitch_tolerance,
    offset_ratio=None  # Ignorer les offsets
)
    
    frame_precision, frame_recall, frame_accuracy = mir_eval.transcription.precision_recall_f1_overlap(
    ref_intervals, ref_pitches, est_intervals, est_pitches,
    offset_ratio=None,  # to calculate accuracy based on onsets and pitches only
    onset_tolerance=0.05,  # adjust tolerance as needed
    pitch_tolerance=0.25  # adjust tolerance as needed
)[0:3]  # to extract only precision, recall, accuracy # changed from mir_eval.transcription_velocity to mir_eval.transcripti
    
    return f_measure, f_measure_no, frame_accuracy



if __name__=="__main__":
    path='C:/Users/admin/Desktop/master2/MLA/Datasets/vocadito/audio/vocadito_9.wav'#Datasets/MTG-QBH/audio projet/C_major_scale.wav
    sample_rate=44100
    f_min=32.7
    n_harmonics=8
    harmonics=[0.5,1,2,3,4,5,6,7]
    hop_length=512
    bins_per_semitone=3
    bins_per_octave=12*bins_per_semitone
    n_bins=bins_per_octave*n_harmonics
    output_freq=500#pas utiliser pour le momment 



    signal,sr=dsp(path)
    cqt_result=cqt(signal,sr,hop_length,f_min,n_bins,bins_per_octave,plot=False)
    print(cqt_result.shape)  # Should give (n_times, n_freqs)

    result=harmonic_stack(cqt_result, sr, harmonics, hop_length, bins_per_semitone,output_freq,plot=False)
    print(result.shape)

    model=model_v1(result.shape)
    model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss={
                'onset': 'binary_crossentropy',
                'note': 'binary_crossentropy',
                'multipitch': 'binary_crossentropy',
            },
            metrics={
                'onset': 'accuracy',
                'note': 'accuracy',
                'multipitch': 'accuracy',
            },
            loss_weights={'onset': 0.95, 'note': 1.0, 'multipitch': 1.0}
        )

    input=np.expand_dims(result, axis=0)
    output=model.predict(input)
    print(output[1][0].shape)

    vis_cqt(output[0][0],sample_rate,hop_length,bins_per_semitone,"Yo",True)
    vis_cqt(output[1][0],sample_rate,hop_length,bins_per_semitone,"Yn",True)
    vis_cqt(output[2][0],sample_rate,hop_length,bins_per_semitone,"Yp",True) 

    Yo, Yn, Yp = output[0][0], output[1][0], output[2][0]
    f_measure, fmeasure_no, frame_accuracy = mir_eval(Yo, Yn, Yp)
    

    
print(f"F-measure: {f_measure:.3f}")
print(f"F-measure (no offset): {f_measure_no:.3f}")
print(f"Frame-level accuracy: {frame_accuracy:.3f}")