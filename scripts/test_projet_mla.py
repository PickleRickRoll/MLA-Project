# -*- coding: utf-8 -*-
"""Test_projet_MLA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W5RJQEwu7wIF3Q4sdnTMCOBDxIc7zYSh
"""

from typing import Any, Callable, Dict
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import ReLU
import tensorflow.keras.layers as nn
from tensorflow.keras.layers import concatenate
#from basic_pitch import nn
#from basic_pitch.constants import SAMPLE_RATE



#On va importer le HCQT car ce n'est pas le but de réapprendre à l'implémenter, mais on peut le completer.


def model_v1(input_shape):
    """
    Builds the lightweight neural network for polyphonic transcription.
    :param input_shape: Tuple indicating the input shape (time, frequency, channels).
    :return: TensorFlow Keras model.
    """
    n_harmonics: int = 8
    n_filters_contour: int = 32
    n_filters_onsets: int = 32
    n_filters_notes: int = 32
    no_contours: bool = False
    CONTOURS_BINS_PER_SEMITONE: int = 12
    N_FREQ_BINS_CONTOURS: int = 13
    # Input layer
    inputs = Input(shape=input_shape)

    #n_harmonics: int = 8,
    """
    #############################################################
    # gestion du harmonic stacking prise dans basic pitch / models.py
    if n_harmonics > 1:
        x = nn.HarmonicStacking(
            CONTOURS_BINS_PER_SEMITONE,
            [0.5] + list(range(1, n_harmonics)),
            N_FREQ_BINS_CONTOURS,
        )(x)
    else:
        x = nn.HarmonicStacking(
            CONTOURS_BINS_PER_SEMITONE,
            [1],
            N_FREQ_BINS_CONTOURS,
        )(x)
      ###############################################################"""

    # First convolutional block (to extract multipitch posteriorgram Yp)
    x_frame = Conv2D(32, (5, 5), padding='same', activation='relu')(inputs)
    x_frame = BatchNormalization()(x_frame)
    x_frame = ReLU()(x_frame)
    x_frame = Conv2D(8, (3, 3*13), padding='same', activation='relu')(x_frame)
    x_frame = BatchNormalization()(x_frame)
    x_frame = ReLU()(x_frame)

    # Output multipitch posteriorgram Yp
    Yp = Conv2D(1, (5, 5), padding='same', activation='sigmoid', name='multipitch')(x_frame)

    # Second block (to extract note posteriorgram Yn using Yp)
    x_note = Conv2D(32, (7, 7), padding='same', activation='relu', strides=(1,3))(Yp)
    x_note = ReLU()(x_note)
    Yn = Conv2D(1, (7, 3), padding='same', activation='sigmoid', name='note')(x_note)

    # Third block (to extract onset posteriorgram Yo using audio features and Yn)
    x_audio = Conv2D(32, (5, 5), padding='same', activation='relu', strides = (1,3))(inputs)
    x_audio = BatchNormalization()(x_audio)
    x_audio = ReLU()(x_audio)
    x_concat = concatenate([x_audio, Yn], axis=3, name='concat')
    Yo = Conv2D(1, (3, 3), padding='same', activation='sigmoid', name='onset')(x_concat)

    # Define the model
    model = Model(inputs, [Yo, Yn, Yp], name="lightweight_AMT")
    return model

# Model summary
input_shape = (200, 60, 1)  # Example input shape (time frames, frequency bins, channels)
model = model_v1(input_shape)

# Compiler le modèle
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss={
        'onset': 'binary_crossentropy',
        'note': 'binary_crossentropy',
        'multipitch': 'binary_crossentropy',
    },
    metrics={
        'onset': 'accuracy',
        'note': 'accuracy',
        'multipitch': 'accuracy',
    },
    loss_weights={'onset': 0.95, 'note': 1.0, 'multipitch': 1.0}
)

# Entraîner le modèle
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=50,  # Ajustez en fonction de vos besoins
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        tf.keras.callbacks.ModelCheckpoint("best_model.h5", save_best_only=True)
    ]
)

"""# Post processing : onsets and frames"""

import numpy as np
from scipy.signal import find_peaks

def detect_onsets(onset_posterior, threshold=0.5):
    """
    Detect onsets using peak picking.
    :param onset_posterior: Array of onset posteriorgram (time, frequency bins).
    :param threshold: Minimum value for a peak to be considered an onset.
    :return: List of onset times.
    """
    # Average across frequency bins to find time-wise peaks
    onset_strength = np.mean(onset_posterior, axis=1)
    peaks, _ = find_peaks(onset_strength, height=threshold, distance=5)
    return peaks

def create_notes(onsets, note_posterior, threshold=0.5, min_duration=3):
    """
    Create note events by tracking active frames from onsets.
    :param onsets: List of onset times (in frames).
    :param note_posterior: Array of note posteriorgram (time, frequency bins).
    :param threshold: Minimum value for a frame to be considered active.
    :param min_duration: Minimum duration of a note (in frames).
    :return: List of notes as (start_frame, end_frame, pitch).
    """
    notes = []
    note_posterior_bin = note_posterior > threshold

    for onset in onsets:
        for pitch in range(note_posterior.shape[1]):
            # Trace forward to find the offset
            start = onset
            end = start
            while end < note_posterior.shape[0] and note_posterior_bin[end, pitch]:
                end += 1
            if end - start >= min_duration:  # Ensure note duration is sufficient
                notes.append((start, end, pitch))

    return notes

def convert_to_time(note_events, hop_size, sr):
    """
    Convert frame-based note events to time-based note events.
    :param note_events: List of (start_frame, end_frame, pitch).
    :param hop_size: Hop size in samples.
    :param sr: Sample rate.
    :return: List of notes as (start_time, end_time, pitch).
    """
    return [(start * hop_size / sr, end * hop_size / sr, pitch) for start, end, pitch in note_events]

def post_process(onset_posterior, note_posterior, pitch_posterior, threshold=0.5, hop_size=256, sr=22050):
    """
    Perform full post-processing to extract note events.
    :param onset_posterior: Onset posteriorgram.
    :param note_posterior: Note posteriorgram.
    :param pitch_posterior: Pitch posteriorgram.
    :param threshold: Threshold for activation.
    :param hop_size: Hop size in samples.
    :param sr: Sample rate.
    :return: List of notes as (onset_time, offset_time, pitch).
    """
    # Step 1: Detect onsets
    onsets = detect_onsets(onset_posterior, threshold)

    # Step 2: Create notes
    notes = create_notes(onsets, note_posterior, threshold)

    # Step 3: Convert frames to time
    note_events = convert_to_time(notes, hop_size, sr)

    return note_events

# Simulations des posteriorgrams
onset_posterior = np.random.rand(1000, 88)  # (time frames, frequency bins)
note_posterior = np.random.rand(1000, 88)
pitch_posterior = np.random.rand(1000, 88)

# Exécution du post-traitement
note_events = post_process(onset_posterior, note_posterior, pitch_posterior)

# Affichage des événements
#for event in note_events:
 #   print(f"Note: Onset at {event[0]:.2f}s, Offset at {event[1]:.2f}s, Pitch: {event[2]}")

"""# Visualisation"""

import librosa.display
import matplotlib.pyplot as plt

def visualize_predictions(audio_path, note_events, sr=22050, hop_length=256):
    """
    Visualize note predictions on the spectrogram.
    :param audio_path: Path to the input audio file.
    :param note_events: List of predicted notes (start_time, end_time, pitch).
    :param sr: Sample rate of the audio.
    :param hop_length: Hop length used in the CQT or spectrogram.
    """
    # Load audio
    audio, _ = librosa.load(audio_path, sr=sr)

    # Compute CQT for visualization
    cqt = librosa.amplitude_to_db(np.abs(librosa.cqt(audio, sr=sr, hop_length=hop_length)), ref=np.max)

    # Plot the CQT
    plt.figure(figsize=(12, 6))
    librosa.display.specshow(cqt, sr=sr, x_axis='time', y_axis='cqt_note', hop_length=hop_length, cmap='coolwarm')

    # Overlay predicted notes
    #for onset, offset, pitch in note_events:
    #   plt.hlines(pitch, onset, offset, color='lime', linestyle='-', linewidth=2)

    plt.title('Note Predictions Over Spectrogram')
    plt.colorbar(label='Amplitude (dB)')
    plt.xlabel('Time (s)')
    plt.ylabel('Frequency (Hz)')
    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))
    plt.show()

!pip install mido

from mido import Message, MidiFile, MidiTrack

def create_midi(note_events, output_path="output.mid", velocity=64):
    """
    Create a MIDI file from note events.
    :param note_events: List of notes as (start_time, end_time, pitch).
    :param output_path: Path to save the MIDI file.
    :param velocity: MIDI velocity (volume) for the notes.
    """
    midi = MidiFile()
    track = MidiTrack()
    midi.tracks.append(track)

    for onset, offset, pitch in note_events:
        onset_tick = int(onset * 480)  # Convert time to ticks
        offset_tick = int(offset * 480)
        track.append(Message('note_on', note=int(pitch), velocity=velocity, time=onset_tick))
        track.append(Message('note_off', note=int(pitch), velocity=0, time=offset_tick - onset_tick))

    midi.save(output_path)
    print(f"MIDI file saved to {output_path}")

note_events = [(0.5, 1.0, 60), (1.2, 2.0, 64)]  # Exemple : [(onset_time, offset_time, pitch)]
create_midi(note_events, "transcription.mid")

!sudo apt-get install fluidsynth
!sudo apt-get install fluid-soundfont-gm

import subprocess

def midi_to_audio(midi_path, output_audio_path, soundfont_path="/usr/share/sounds/sf2/FluidR3_GM.sf2"):
    """
    Convert a MIDI file to audio using FluidSynth.
    :param midi_path: Path to the input MIDI file.
    :param output_audio_path: Path to save the synthesized audio.
    :param soundfont_path: Path to the soundfont file (SF2).
    """
    command = ["fluidsynth", "-ni", soundfont_path, midi_path, "-F", output_audio_path]
    subprocess.run(command)
    print(f"Audio file saved to {output_audio_path}")

midi_to_audio("transcription.mid", "synthesized_audio.wav")

# Étape 1 : Charger audio et obtenir prédictions
onset_posterior = np.random.rand(1000, 88)  # Simulations
note_posterior = np.random.rand(1000, 88)
pitch_posterior = np.random.rand(1000, 88)

note_events = post_process(onset_posterior, note_posterior, pitch_posterior)

# Étape 2 : Visualiser les prédictions
visualize_predictions("synthesized_audio.wav", note_events)

# Étape 3 : Générer MIDI
create_midi(note_events, "transcription.mid")

# Étape 4 : Générer audio
midi_to_audio("transcription.mid", "transcription_audio.wav")

import matplotlib.pyplot as plt
import librosa.display
import numpy as np

def visualize_hcqt(hcqt, note_events, sr=22050, hop_length=256):
    """
    Visualize the HCQT representation with superimposed note events.
    :param hcqt: HCQT array (time, frequency, harmonics).
    :param note_events: List of predicted notes (onset_time, offset_time, pitch).
    :param sr: Sample rate of the audio.
    :param hop_length: Hop length used in HCQT.
    """
    # Combine harmonics by averaging across harmonic dimensions
    combined_hcqt = np.mean(hcqt, axis=-1)  # Shape: (time, frequency)

    # Convert HCQT to dB
    combined_hcqt_db = librosa.amplitude_to_db(combined_hcqt, ref=np.max)

    # Plot HCQT
    plt.figure(figsize=(12, 6))
    librosa.display.specshow(combined_hcqt_db, sr=sr, x_axis='time', y_axis='cqt_note', hop_length=hop_length, cmap='magma')

    # Superimpose note events
    for onset, offset, pitch in note_events:
        plt.hlines(pitch, onset, offset, color='lime', linestyle='-', linewidth=2, label=f'Note {pitch}')

    plt.colorbar(label='Amplitude (dB)')
    plt.title('HCQT with Note Predictions')
    plt.xlabel('Time (s)')
    plt.ylabel('Frequency (Hz)')
    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))
    plt.show()

!pip install basic-pitch

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
#from basic_pitch import compute_hcqt # Import compute_hcqt from basic_pitch



def preprocess_audio(audio_path, sr=22050, hop_length=256, n_harmonics=8):
    """
    Preprocesses the audio file to generate the HCQT input.

    :param audio_path: Path to the audio file.
    :param sr: Sample rate.
    :param hop_length: Hop length for HCQT.
    :param n_harmonics: Number of harmonics to consider in HCQT.
    :return: HCQT representation of the audio.
    """
    # Load audio
    y, _ = librosa.load(audio_path, sr=sr)


    # Calculate HCQT using basic_pitch
    # basic_pitch expects a mono signal. Make sure y is mono
    if y.ndim > 1:
        y = librosa.to_mono(y)

    # compute_hcqt returns a tensor of shape (batch, time, freq, harmonic)
    # Assuming you're processing a single file (batch size = 1)
    #hcqt = compute_hcqt(y[None, :], sr, hop_length=hop_length, harmonics=n_harmonics)

    # Squeeze the batch dimension to get (time, freq, harmonic)
    #hcqt = hcqt.squeeze(0).numpy()

    # Calculate HCQT
    bins_per_octave = 12 * n_harmonics
    hcqt = librosa.feature.chroma_cqt(y=y,
     sr=sr,
      hop_length=hop_length,
       n_chroma=n_harmonics,
       bins_per_octave=bins_per_octave)

    if hcqt.ndim == 2:
        hcqt = hcqt[..., np.newaxis]  # Add a dimension for harmonics


    return hcqt

# Charger et prétraiter l'audio pour obtenir le HCQT
audio_path = "transcription_audio.wav"
hcqt_input = preprocess_audio(audio_path)

# Note_events simulés
note_events = [(0.5, 1.0, 60), (1.2, 2.0, 64)]

# Visualiser la HCQT et les prédictions
visualize_hcqt(hcqt_input, note_events)

# Charger et prétraiter l'audio pour obtenir le HCQT
audio_path = "transcription_audio.wav"
hcqt_input = preprocess_audio(audio_path)

# Note_events simulés
note_events = [(0.5, 1.0, 60), (1.2, 2.0, 64)]

# Visualiser la HCQT et les prédictions
visualize_hcqt(hcqt_input, note_events)

def reconstruct_audio_from_hcqt(hcqt, sr=22050, hop_length=256):
    """
    Reconstruct audio from the HCQT by using the fundamental (base) CQT.
    :param hcqt: HCQT array (time, frequency, harmonics).
    :param sr: Sample rate of the audio.
    :param hop_length: Hop length used in HCQT.
    :return: Reconstructed audio signal.
    """
    # Use the fundamental harmonic (base CQT)
    cqt_base = hcqt[..., 0]  # Shape: (time, frequency)

    # Convert to linear amplitude scale
    cqt_base_amplitude = np.abs(cqt_base)

    # Reconstruct audio using iCQT
    audio_reconstructed = librosa.icqt(cqt_base_amplitude, sr=sr, hop_length=hop_length, bins_per_octave=36)
    return audio_reconstructed

import soundfile as sf

# Reconstruire et sauvegarder l’audio
audio_reconstructed = reconstruct_audio_from_hcqt(hcqt_input)
sf.write("reconstructed_audio.wav", audio_reconstructed, samplerate=22050)

# Charger et prétraiter l'audio pour obtenir HCQT
audio_path = "example.wav"
hcqt_input = preprocess_audio(audio_path)

# Notes simulées ou générées par le modèle
note_events = [(0.5, 1.0, 60), (1.2, 2.0, 64)]

# Étape 1 : Visualisation HCQT avec prédictions
visualize_hcqt(hcqt_input, note_events)

# Étape 2 : Reconstruction audio depuis HCQT
audio_reconstructed = reconstruct_audio_from_hcqt(hcqt_input)

# Étape 3 : Sauvegarde du fichier audio reconstruit
sf.write("reconstructed_audio.wav", audio_reconstructed, samplerate=22050)